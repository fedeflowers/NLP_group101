{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77650b3",
   "metadata": {},
   "source": [
    "# 1st task, dataset exploration and binary calssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc72f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Reusing dataset parquet (C:\\Users\\fiori\\.cache\\huggingface\\datasets\\copenlu___parquet\\copenlu--nlp_course_tydiqa-cceecfb5416d988a\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4236bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(train_set)#.sample(frac=1) #RESHUFFLING\n",
    "vs = pd.DataFrame(validation_set)#.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166d770d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116062</th>\n",
       "      <td>Kapan Kaisar Tang Gaozu mulai menjabat ?</td>\n",
       "      <td>Kaisar Tang Gaozu</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Hingga tahun 626, situasi makin memanas, Li Sh...</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Kaisar%20Tang%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116063</th>\n",
       "      <td>من ابتكر المثلجات؟</td>\n",
       "      <td>مثلجات</td>\n",
       "      <td>arabic</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>وأشار هؤلاء الإخصائيون إلى أن “صداع الآيس كريم...</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D9%85%D8%AB%D9%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116064</th>\n",
       "      <td>বাংলা ব্যাকরণ মতে বিশেষণ কয় প্রকার ?</td>\n",
       "      <td>বিশেষণ</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>উপমান: যার সাথে তুলনা করা হয়।\\nউপমেয়: যাকে ত...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116065</th>\n",
       "      <td>ブラームスの出身はどこ？</td>\n",
       "      <td>ヨハネス・ブラームス</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116066</th>\n",
       "      <td>What is the population of Mahwah, NJ?</td>\n",
       "      <td>Mahwah, New Jersey</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>The previous mayor, Bill Laforet faced a recal...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahwah%2C%20New%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116067 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question_text      document_title  \\\n",
       "0                   Milloin Charles Fort syntyi?        Charles Fort   \n",
       "1                    “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ        ダニエル・J・キャラハン   \n",
       "2         వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?                 వేప   \n",
       "3             চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?         চেঙ্গিজ খান   \n",
       "4               రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?          రెయ్యలగడ్ద   \n",
       "...                                          ...                 ...   \n",
       "116062  Kapan Kaisar Tang Gaozu mulai menjabat ?   Kaisar Tang Gaozu   \n",
       "116063                        من ابتكر المثلجات؟              مثلجات   \n",
       "116064      বাংলা ব্যাকরণ মতে বিশেষণ কয় প্রকার ?              বিশেষণ   \n",
       "116065                              ブラームスの出身はどこ？          ヨハネス・ブラームス   \n",
       "116066     What is the population of Mahwah, NJ?  Mahwah, New Jersey   \n",
       "\n",
       "          language                                        annotations  \\\n",
       "0          finnish  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1         japanese  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2           telugu  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3          bengali  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4           telugu  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "...            ...                                                ...   \n",
       "116062  indonesian        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116063      arabic        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116064     bengali        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116065    japanese        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116066     english        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                       document_plaintext  \\\n",
       "0       Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1       “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2       వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3       চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4       రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "...                                                   ...   \n",
       "116062  Hingga tahun 626, situasi makin memanas, Li Sh...   \n",
       "116063  وأشار هؤلاء الإخصائيون إلى أن “صداع الآيس كريم...   \n",
       "116064  উপমান: যার সাথে তুলনা করা হয়।\\nউপমেয়: যাকে ত...   \n",
       "116065  \\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...   \n",
       "116066  The previous mayor, Bill Laforet faced a recal...   \n",
       "\n",
       "                                             document_url  \n",
       "0            https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1       https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2       https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3       https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4       https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  \n",
       "...                                                   ...  \n",
       "116062  https://id.wikipedia.org/wiki/Kaisar%20Tang%20...  \n",
       "116063  https://ar.wikipedia.org/wiki/%D9%85%D8%AB%D9%...  \n",
       "116064  https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...  \n",
       "116065  https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...  \n",
       "116066  https://en.wikipedia.org/wiki/Mahwah%2C%20New%...  \n",
       "\n",
       "[116067 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c561824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "new_vs = []\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i,2] == 'japanese' or df.iloc[i,2] == 'finnish' or df.iloc[i,2] == 'english':\n",
    "        new_df.append(df.iloc[i, :])\n",
    "for i in range(len(vs)):\n",
    "    if vs.iloc[i,2] == 'japanese' or vs.iloc[i,2] == 'finnish' or vs.iloc[i,2] == 'english':\n",
    "        new_vs.append(vs.iloc[i, :])\n",
    "df = pd.DataFrame(new_df) #only with english, japanese or finnish text\n",
    "vs = pd.DataFrame(new_vs) #only with english, japanese or finnish text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5595f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mitä on altruismi?</td>\n",
       "      <td>Altruismi</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [44], 'answer_text': ['epäits...</td>\n",
       "      <td>\\n\\n\\nAltruismi ([1],  ”toinen”[2]) tarkoittaa...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Altruismi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mikä oli Wilhelm Wagner viimeinen sävellys?</td>\n",
       "      <td>Richard Wagner</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [597], 'answer_text': ['Parsi...</td>\n",
       "      <td>Wagnerin mestariteoksia ovat hänen myöhäiskaud...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Richard%20Wagner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Missä Harz sijaitsee?</td>\n",
       "      <td>Harz</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [25], 'answer_text': ['Pohjoi...</td>\n",
       "      <td>\\n\\nHarz on horstivuoristo Pohjois-Saksassa[1]...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Harz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116055</th>\n",
       "      <td>Who developed the first thermonuclear weapon?</td>\n",
       "      <td>History of nuclear weapons</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>In the end, President Truman made the final de...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/History%20of%20n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116059</th>\n",
       "      <td>ギュスターヴ・シャルパンティエはいつ生まれた？</td>\n",
       "      <td>ギュスターヴ・シャルパンティエ</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>ただし、第一次世界大戦時に負傷兵のための音楽会を主宰し、自作の指揮も行うなど隠棲することはな...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%AE%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116060</th>\n",
       "      <td>彭 徳懐はいつ生まれた？</td>\n",
       "      <td>彭徳懐故居</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>彭徳懐故居（ほうとくかいこきょ）は、中華人民共和国の政治家で軍人（中華人民共和国元帥）だった...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%BD%AD%E5%BE%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116065</th>\n",
       "      <td>ブラームスの出身はどこ？</td>\n",
       "      <td>ヨハネス・ブラームス</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116066</th>\n",
       "      <td>What is the population of Mahwah, NJ?</td>\n",
       "      <td>Mahwah, New Jersey</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>The previous mayor, Bill Laforet faced a recal...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahwah%2C%20New%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29868 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_text  \\\n",
       "0                        Milloin Charles Fort syntyi?   \n",
       "1                         “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   \n",
       "10                                 Mitä on altruismi?   \n",
       "12        Mikä oli Wilhelm Wagner viimeinen sävellys?   \n",
       "13                              Missä Harz sijaitsee?   \n",
       "...                                               ...   \n",
       "116055  Who developed the first thermonuclear weapon?   \n",
       "116059                        ギュスターヴ・シャルパンティエはいつ生まれた？   \n",
       "116060                                   彭 徳懐はいつ生まれた？   \n",
       "116065                                   ブラームスの出身はどこ？   \n",
       "116066          What is the population of Mahwah, NJ?   \n",
       "\n",
       "                    document_title  language  \\\n",
       "0                     Charles Fort   finnish   \n",
       "1                     ダニエル・J・キャラハン  japanese   \n",
       "10                       Altruismi   finnish   \n",
       "12                  Richard Wagner   finnish   \n",
       "13                            Harz   finnish   \n",
       "...                            ...       ...   \n",
       "116055  History of nuclear weapons   english   \n",
       "116059             ギュスターヴ・シャルパンティエ  japanese   \n",
       "116060                       彭徳懐故居  japanese   \n",
       "116065                  ヨハネス・ブラームス  japanese   \n",
       "116066          Mahwah, New Jersey   english   \n",
       "\n",
       "                                              annotations  \\\n",
       "0       {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1       {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "10      {'answer_start': [44], 'answer_text': ['epäits...   \n",
       "12      {'answer_start': [597], 'answer_text': ['Parsi...   \n",
       "13      {'answer_start': [25], 'answer_text': ['Pohjoi...   \n",
       "...                                                   ...   \n",
       "116055        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116059        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116060        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116065        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116066        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                       document_plaintext  \\\n",
       "0       Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1       “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "10      \\n\\n\\nAltruismi ([1],  ”toinen”[2]) tarkoittaa...   \n",
       "12      Wagnerin mestariteoksia ovat hänen myöhäiskaud...   \n",
       "13      \\n\\nHarz on horstivuoristo Pohjois-Saksassa[1]...   \n",
       "...                                                   ...   \n",
       "116055  In the end, President Truman made the final de...   \n",
       "116059  ただし、第一次世界大戦時に負傷兵のための音楽会を主宰し、自作の指揮も行うなど隠棲することはな...   \n",
       "116060  彭徳懐故居（ほうとくかいこきょ）は、中華人民共和国の政治家で軍人（中華人民共和国元帥）だった...   \n",
       "116065  \\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...   \n",
       "116066  The previous mayor, Bill Laforet faced a recal...   \n",
       "\n",
       "                                             document_url  \n",
       "0            https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1       https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "10                https://fi.wikipedia.org/wiki/Altruismi  \n",
       "12         https://fi.wikipedia.org/wiki/Richard%20Wagner  \n",
       "13                     https://fi.wikipedia.org/wiki/Harz  \n",
       "...                                                   ...  \n",
       "116055  https://en.wikipedia.org/wiki/History%20of%20n...  \n",
       "116059  https://ja.wikipedia.org/wiki/%E3%82%AE%E3%83%...  \n",
       "116060  https://ja.wikipedia.org/wiki/%E5%BD%AD%E5%BE%...  \n",
       "116065  https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...  \n",
       "116066  https://en.wikipedia.org/wiki/Mahwah%2C%20New%...  \n",
       "\n",
       "[29868 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "288ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer as tok_jap #japanese tokenizer\n",
    "tok_jap = tok_jap()\n",
    "from nltk.tokenize import word_tokenize #english tokenizer\n",
    "#[token for token in t.tokenize(df.iloc[1,4], wakati=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d9b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 29868/29868 [02:22<00:00, 209.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_answers = {} #dictionary, key = number of iteration in the dataframe, value = answer_text, or '' if it's not answerable\n",
    "tokenized_questions = {} #same as before but with questions\n",
    "tokenized_documents = {} #same but with documents\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    if df.iloc[i,2] == 'japanese': #check the language\n",
    "        tokenized_answers[i] = [token for token in tok_jap.tokenize(df.iloc[i,3]['answer_text'][0], wakati=True)]\n",
    "        tokenized_questions[i] = [token for token in tok_jap.tokenize(df.iloc[i,0], wakati=True)]\n",
    "        tokenized_documents[i] = [token for token in tok_jap.tokenize(df.iloc[i,4], wakati=True)]\n",
    "    if df.iloc[i,2] == 'english':\n",
    "        tokenized_answers[i] = word_tokenize(df.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions[i] = word_tokenize(df.iloc[i,0])\n",
    "        tokenized_documents[i] = word_tokenize(df.iloc[i,4])\n",
    "    if df.iloc[i,2] == 'finnish': #I used the same tokenizer, maybe we can find another one that is better for finnish\n",
    "        tokenized_answers[i] = word_tokenize(df.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions[i] = word_tokenize(df.iloc[i,0])\n",
    "        tokenized_documents[i] = word_tokenize(df.iloc[i,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ef9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3712/3712 [00:16<00:00, 229.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_answers_validation = {} #dictionary, key = number of iteration in the dataframe, value = answer_text, or '' if it's not answerable\n",
    "tokenized_questions_validation = {} #same as before but with questions\n",
    "tokenized_documents_validation = {} #same but with documents\n",
    "\n",
    "for i in tqdm(range(len(vs))):\n",
    "    if vs.iloc[i,2] == 'japanese': #check the language\n",
    "        tokenized_answers_validation[i] = [token for token in tok_jap.tokenize(vs.iloc[i,3]['answer_text'][0], wakati=True)]\n",
    "        tokenized_questions_validation[i] = [token for token in tok_jap.tokenize(vs.iloc[i,0], wakati=True)]\n",
    "        tokenized_documents_validation[i] = [token for token in tok_jap.tokenize(vs.iloc[i,4], wakati=True)]\n",
    "    if vs.iloc[i,2] == 'english':\n",
    "        tokenized_answers_validation[i] = word_tokenize(vs.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions_validation[i] = word_tokenize(vs.iloc[i,0])\n",
    "        tokenized_documents_validation[i] = word_tokenize(vs.iloc[i,4])\n",
    "    if vs.iloc[i,2] == 'finnish': #I used the same tokenizer, maybe we can find another one that is better for finnish\n",
    "        tokenized_answers_validation[i] = word_tokenize(vs.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions_validation[i] = word_tokenize(vs.iloc[i,0])\n",
    "        tokenized_documents_validation[i] = word_tokenize(vs.iloc[i,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a9203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>化学兵器禁止条約はどこで採択された？</td>\n",
       "      <td>化学兵器禁止条約</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [11], 'answer_text': ['パリ']}</td>\n",
       "      <td>1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>オリヴィア・デ・ハヴィランドが生まれたのはいつ</td>\n",
       "      <td>オリヴィア・デ・ハヴィランド</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [46], 'answer_text': ['1916年7...</td>\n",
       "      <td>\\nオリヴィア・デ・ハヴィランド（Dame Olivia De Havilland, DBE...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kauanko lasia on valmistettu?</td>\n",
       "      <td>Lasi</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [160], 'answer_text': ['noin ...</td>\n",
       "      <td>Vanhin tunnettu lasilaatu on alkali­kalkki­las...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Lasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mikä on Ponzi-huijaus?</td>\n",
       "      <td>Ponzi-huijaus</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [39], 'answer_text': ['pyrami...</td>\n",
       "      <td>Ponzi-huijaus eli Ponzi-järjestelmä on pyramid...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Ponzi-huijaus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mikä oli Napoleonin sotien lopputulos?</td>\n",
       "      <td>Napoleonin sodat</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [15], 'answer_text': ['Ranska...</td>\n",
       "      <td>Sotien jälkeen Ranska alistettiin kovilla rauh...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Napoleonin%20sodat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>What is the most common first word by babies?</td>\n",
       "      <td>Vocabulary development</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Social pragmatic theories, also in contrast to...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vocabulary%20dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>Kuinka kauan valtiopäivät kestää?</td>\n",
       "      <td>Suomen valtiopäivät</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nEnnen vuotta 1906 kokoontuivat säätyvaltiopä...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Suomen%20valtiop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13315</th>\n",
       "      <td>アイスランド共和国の首都はどこですか？</td>\n",
       "      <td>アイスランド</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>こうした危機を乗り切るため、アイスランド中央銀行は8日にロシアから40億ユーロの緊急融資を受...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>Milloin käytiin Persianlahden sota?</td>\n",
       "      <td>Persianlahden sota</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Ilmaiskuilla oli erityisen tuhoisa vaikutus Ir...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Persianlahden%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13319</th>\n",
       "      <td>When did the Bundaberg Central State School be...</td>\n",
       "      <td>Bundaberg Central State School</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>By the 1880s the school site had become valuab...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bundaberg%20Cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3712 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question_text  \\\n",
       "3                                     化学兵器禁止条約はどこで採択された？   \n",
       "9                                オリヴィア・デ・ハヴィランドが生まれたのはいつ   \n",
       "11                         Kauanko lasia on valmistettu?   \n",
       "14                                Mikä on Ponzi-huijaus?   \n",
       "20                Mikä oli Napoleonin sotien lopputulos?   \n",
       "...                                                  ...   \n",
       "13305      What is the most common first word by babies?   \n",
       "13307                  Kuinka kauan valtiopäivät kestää?   \n",
       "13315                                アイスランド共和国の首都はどこですか？   \n",
       "13317                Milloin käytiin Persianlahden sota?   \n",
       "13319  When did the Bundaberg Central State School be...   \n",
       "\n",
       "                       document_title  language  \\\n",
       "3                            化学兵器禁止条約  japanese   \n",
       "9                      オリヴィア・デ・ハヴィランド  japanese   \n",
       "11                               Lasi   finnish   \n",
       "14                      Ponzi-huijaus   finnish   \n",
       "20                   Napoleonin sodat   finnish   \n",
       "...                               ...       ...   \n",
       "13305          Vocabulary development   english   \n",
       "13307             Suomen valtiopäivät   finnish   \n",
       "13315                          アイスランド  japanese   \n",
       "13317              Persianlahden sota   finnish   \n",
       "13319  Bundaberg Central State School   english   \n",
       "\n",
       "                                             annotations  \\\n",
       "3          {'answer_start': [11], 'answer_text': ['パリ']}   \n",
       "9      {'answer_start': [46], 'answer_text': ['1916年7...   \n",
       "11     {'answer_start': [160], 'answer_text': ['noin ...   \n",
       "14     {'answer_start': [39], 'answer_text': ['pyrami...   \n",
       "20     {'answer_start': [15], 'answer_text': ['Ranska...   \n",
       "...                                                  ...   \n",
       "13305        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13307        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13315        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13317        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13319        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "3      1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...   \n",
       "9      \\nオリヴィア・デ・ハヴィランド（Dame Olivia De Havilland, DBE...   \n",
       "11     Vanhin tunnettu lasilaatu on alkali­kalkki­las...   \n",
       "14     Ponzi-huijaus eli Ponzi-järjestelmä on pyramid...   \n",
       "20     Sotien jälkeen Ranska alistettiin kovilla rauh...   \n",
       "...                                                  ...   \n",
       "13305  Social pragmatic theories, also in contrast to...   \n",
       "13307  \\nEnnen vuotta 1906 kokoontuivat säätyvaltiopä...   \n",
       "13315  こうした危機を乗り切るため、アイスランド中央銀行は8日にロシアから40億ユーロの緊急融資を受...   \n",
       "13317  Ilmaiskuilla oli erityisen tuhoisa vaikutus Ir...   \n",
       "13319  By the 1880s the school site had become valuab...   \n",
       "\n",
       "                                            document_url  \n",
       "3      https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...  \n",
       "9      https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%...  \n",
       "11                    https://fi.wikipedia.org/wiki/Lasi  \n",
       "14           https://fi.wikipedia.org/wiki/Ponzi-huijaus  \n",
       "20      https://fi.wikipedia.org/wiki/Napoleonin%20sodat  \n",
       "...                                                  ...  \n",
       "13305  https://en.wikipedia.org/wiki/Vocabulary%20dev...  \n",
       "13307  https://fi.wikipedia.org/wiki/Suomen%20valtiop...  \n",
       "13315  https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%...  \n",
       "13317  https://fi.wikipedia.org/wiki/Persianlahden%20...  \n",
       "13319  https://en.wikipedia.org/wiki/Bundaberg%20Cent...  \n",
       "\n",
       "[3712 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194d9f8",
   "metadata": {},
   "source": [
    "# Creating trainset and validation set for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b554f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#need only question and document, I don't really need the tokenization done before:\n",
    "jap_training = {} \n",
    "finn_training = {}\n",
    "eng_training = {}\n",
    "\n",
    "j = 0\n",
    "f = 0\n",
    "e = 0\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sample_dict = {}\n",
    "    sample_dict['question'] = tokenized_questions[i]\n",
    "    sample_dict['context'] = tokenized_documents[i]\n",
    "   \n",
    "    #sample_dict['question'] = df.iloc[i,0]\n",
    "    #sample_dict['document'] = df.iloc[i,4]\n",
    "    \n",
    "    if tokenized_answers[i] == [] :\n",
    "        sample_dict['label'] = -1 #not answerable question\n",
    "    else:\n",
    "        sample_dict['label'] = 1\n",
    "        \n",
    "    if df.iloc[i,2] == 'japanese':\n",
    "        jap_training[j] = sample_dict\n",
    "        j += 1\n",
    "        continue\n",
    "    elif df.iloc[i,2] == 'english':\n",
    "        eng_training[e] = sample_dict\n",
    "        e += 1\n",
    "        continue\n",
    "    elif df.iloc[i,2] == 'finnish':\n",
    "        finn_training[f] = sample_dict\n",
    "        f += 1\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7562423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation set\n",
    "import nltk\n",
    "\n",
    "jap_val = {} \n",
    "finn_val = {}\n",
    "eng_val = {}\n",
    "\n",
    "j = 0\n",
    "f = 0\n",
    "e = 0\n",
    "\n",
    "\n",
    "for i in range(len(vs)):\n",
    "    sample_dict = {}\n",
    "    sample_dict['question'] = tokenized_questions_validation[i]\n",
    "    sample_dict['context'] = tokenized_documents_validation[i]\n",
    "    \n",
    "    if tokenized_answers_validation[i] == [] :\n",
    "        sample_dict['label'] = -1 #not answerable question\n",
    "    else:\n",
    "        sample_dict['label'] = 1\n",
    "        \n",
    "    if vs.iloc[i,2] == 'japanese':\n",
    "        jap_val[j] = sample_dict\n",
    "        j += 1\n",
    "        continue\n",
    "    elif vs.iloc[i,2] == 'english':\n",
    "        eng_val[e] = sample_dict\n",
    "        e += 1\n",
    "        continue\n",
    "    elif vs.iloc[i,2] == 'finnish':\n",
    "        finn_val[f] = sample_dict\n",
    "        f += 1\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3583e",
   "metadata": {},
   "source": [
    "## Create corpus for tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da71b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus to create a vocabulary with all possible words\n",
    "\n",
    "corpus_questions_eng = []\n",
    "corpus_questions_finn = []\n",
    "corpus_questions_jap = []\n",
    "corpus_context_eng = []\n",
    "corpus_context_finn = []\n",
    "corpus_context_jap = []\n",
    "\n",
    "for x in eng_training:\n",
    "    corpus_questions_eng.append(eng_training[x]['question'])\n",
    "    corpus_context_eng.append(eng_training[x]['context'])\n",
    "for x in finn_training:\n",
    "    corpus_questions_finn.append(finn_training[x]['question'])\n",
    "    corpus_context_finn.append(finn_training[x]['context'])\n",
    "for x in jap_training:\n",
    "    corpus_questions_jap.append(jap_training[x]['question'])\n",
    "    corpus_context_jap.append(jap_training[x]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8ac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_questions_VAL_eng = []\n",
    "corpus_context_VAL_eng = []\n",
    "corpus_questions_VAL_finn = []\n",
    "corpus_context_VAL_finn = []\n",
    "corpus_questions_VAL_jap = []\n",
    "corpus_context_VAL_jap = []\n",
    "\n",
    "for x in eng_val:\n",
    "    corpus_questions_VAL_eng.append(eng_val[x]['question'])  \n",
    "    corpus_context_VAL_eng.append(eng_val[x]['context']) \n",
    "for x in finn_val:\n",
    "    corpus_questions_VAL_finn.append(finn_val[x]['question'])  \n",
    "    corpus_context_VAL_finn.append(finn_val[x]['context']) \n",
    "for x in jap_val:\n",
    "    corpus_questions_VAL_jap.append(jap_val[x]['question'])  \n",
    "    corpus_context_VAL_jap.append(jap_val[x]['context']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec0d4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_dict_eng = {}\n",
    "bag_of_words_dict_finn = {}\n",
    "bag_of_words_dict_jap = {}\n",
    "\n",
    "for sent in corpus_questions_eng:\n",
    "    for word in sent:\n",
    "        if word not in bag_of_words_dict_eng:\n",
    "            bag_of_words_dict_eng[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_eng[word] +=1\n",
    "            \n",
    "for question in corpus_questions_finn:\n",
    "    for word in question:\n",
    "        if word not in bag_of_words_dict_finn:\n",
    "            bag_of_words_dict_finn[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_finn[word] +=1\n",
    "            \n",
    "for question in corpus_questions_jap:\n",
    "    for word in question:\n",
    "        if word not in bag_of_words_dict_jap:\n",
    "            bag_of_words_dict_jap[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_jap[word] +=1\n",
    "            \n",
    "#list_words = bag_of_words_dict_eng.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed1de7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbag_of_words_dict_eng = {}\\nfor question in corpus_questions:\\n    for word in question:\\n        if word not in bag_of_words_dict_eng:\\n            bag_of_words_dict_eng[word] = 1\\n        else:\\n            bag_of_words_dict_eng[word] +=1\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VALIDATIO SET, the vocabulary should be the same as the training set\n",
    "#I have to use the same vocabulary as the training, even for the validation set, that's why the commented part of code it's not useful\n",
    "'''\n",
    "bag_of_words_dict_eng = {}\n",
    "for question in corpus_questions:\n",
    "    for word in question:\n",
    "        if word not in bag_of_words_dict_eng:\n",
    "            bag_of_words_dict_eng[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_eng[word] +=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b566f53",
   "metadata": {},
   "source": [
    "### Top 10 words in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af81be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'the', 'When', 'was', 'What', 'is', 'of', 'How', 'in', 'did']\n"
     ]
    }
   ],
   "source": [
    "sorted_words_question = dict(sorted(bag_of_words_dict_eng.items(), key=lambda item:item[1], \n",
    "reverse=True))\n",
    "print(list(sorted_words_question)[:10]) #top 10 words in english questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fbfe2",
   "metadata": {},
   "source": [
    "### Top 10 words in finnish questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f76aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'on', 'Milloin', 'Mikä', 'Missä', 'Kuka', 'oli', 'Mitä', 'syntyi', 'kuoli']\n"
     ]
    }
   ],
   "source": [
    "sorted_words_question = dict(sorted(bag_of_words_dict_finn.items(), key=lambda item:item[1], \n",
    "reverse=True))\n",
    "print(list(sorted_words_question)[:10]) #top 10 words in english questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7f9b2",
   "metadata": {},
   "source": [
    "### Top 10 words in japanese questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58fde165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['は', 'の', '？', 'た', 'い', 'つ', '何', 'し', 'どこ', 'が']\n"
     ]
    }
   ],
   "source": [
    "sorted_words_question = dict(sorted(bag_of_words_dict_jap.items(), key=lambda item:item[1], \n",
    "reverse=True))\n",
    "print(list(sorted_words_question)[:10]) #top 10 words in english questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa41cd2",
   "metadata": {},
   "source": [
    "# TfIdf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd589a20",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90d14ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_questions_eng_not_tokenized = [\" \".join(x) for x in corpus_questions_eng]\n",
    "corpus_context_eng_not_tokenized = [\" \".join(x) for x in corpus_context_eng]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2184ff",
   "metadata": {},
   "source": [
    "##### use tfidf for questions and then for context, concatenate the 2 and try to predict, for validation set use the same model of tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e549d070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<7389x4603 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 50118 stored elements in Compressed Sparse Row format>,\n",
       " <7389x50037 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 455794 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_questions = TfidfVectorizer()\n",
    "q = tfidf_vectorizer_questions.fit_transform(corpus_questions_eng_not_tokenized)\n",
    "\n",
    "tfidf_vectorizer_context = TfidfVectorizer() #new model\n",
    "c = tfidf_vectorizer_context.fit_transform(corpus_context_eng_not_tokenized) \n",
    "q,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be05b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([q,c]) #questions and contexts tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "006e950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7389x54640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 505912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1baaa9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why not use fit_transform also on validation set:\n",
    "#https://stats.stackexchange.com/questions/154660/tfidfvectorizer-should-it-be-used-on-train-only-or-traintest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aac13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat process for validation set:\n",
    "corpus_questions_VAL_eng_not_t = [\" \".join(x) for x in corpus_questions_VAL_eng] # not tokenized\n",
    "corpus_context_VAL_eng_not_t = [\" \".join(x) for x in corpus_context_VAL_eng]\n",
    "q = tfidf_vectorizer_questions.transform(corpus_questions_VAL_eng_not_t) #ignores unknown words\n",
    "c = tfidf_vectorizer_context.transform(corpus_context_VAL_eng_not_t) #ignores unknown words\n",
    "\n",
    "X_VAL = hstack([q,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9de2389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<990x54640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 63594 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367f0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [] #-1 not answ, 1 is answ.\n",
    "Y_VAL = []\n",
    "for i in eng_training:\n",
    "    y.append(eng_training[i]['label'])\n",
    "    \n",
    "for i in eng_val:\n",
    "    Y_VAL.append(eng_val[i]['label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "741fe620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "89.21369603464609\n",
      "vald_set accuracy\n",
      "72.42424242424242\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "#training accuracy score\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59329d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_eng = X\n",
    "features_val_eng = X_VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45dc31",
   "metadata": {},
   "source": [
    "# Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c17406dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "91.69403693161084\n",
      "vald_set accuracy\n",
      "71.23368920521945\n"
     ]
    }
   ],
   "source": [
    "#same as english\n",
    "corpus_questions_finn_not_tokenized = [\" \".join(x) for x in corpus_questions_finn]\n",
    "corpus_context_finn_not_tokenized = [\" \".join(x) for x in corpus_context_finn]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_questions = TfidfVectorizer()\n",
    "q = tfidf_vectorizer_questions.fit_transform(corpus_questions_finn_not_tokenized)\n",
    "\n",
    "tfidf_vectorizer_context = TfidfVectorizer() #new model\n",
    "c = tfidf_vectorizer_context.fit_transform(corpus_context_finn_not_tokenized) \n",
    "\n",
    "#concatenate sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([q,c]) #questions and contexts tfidfs\n",
    "\n",
    "corpus_questions_VAL_finn_not_t = [\" \".join(x) for x in corpus_questions_VAL_finn] # not tokenized\n",
    "corpus_context_VAL_finn_not_t = [\" \".join(x) for x in corpus_context_VAL_finn]\n",
    "q = tfidf_vectorizer_questions.transform(corpus_questions_VAL_finn_not_t) #ignores unknown words\n",
    "c = tfidf_vectorizer_context.transform(corpus_context_VAL_finn_not_t) #ignores unknown words\n",
    "\n",
    "X_VAL = hstack([q,c])\n",
    "\n",
    "y = [] #-1 not answ, 1 is answ.\n",
    "Y_VAL = []\n",
    "for i in finn_training:\n",
    "    y.append(finn_training[i]['label'])\n",
    "    \n",
    "for i in finn_val:\n",
    "    Y_VAL.append(finn_val[i]['label'])\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "#training accuracy score\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f8e79",
   "metadata": {},
   "source": [
    "# Japanese\n",
    "##### overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bef4230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "98.32535885167464\n",
      "vald_set accuracy\n",
      "57.91505791505791\n"
     ]
    }
   ],
   "source": [
    "#same as english\n",
    "corpus_questions_jap_not_tokenized = [\"\".join(x) for x in corpus_questions_jap] # no space in japanese\n",
    "corpus_context_jap_not_tokenized = [\"\".join(x) for x in corpus_context_jap]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_questions = TfidfVectorizer()\n",
    "q = tfidf_vectorizer_questions.fit_transform(corpus_questions_jap_not_tokenized)\n",
    "\n",
    "tfidf_vectorizer_context = TfidfVectorizer()\n",
    "c = tfidf_vectorizer_context.fit_transform(corpus_context_jap_not_tokenized) \n",
    "\n",
    "#concatenate sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([q,c]) #questions and contexts tfidfs\n",
    "\n",
    "corpus_questions_VAL_jap_not_t = [\" \".join(x) for x in corpus_questions_VAL_jap] # not tokenized\n",
    "corpus_context_VAL_jap_not_t = [\" \".join(x) for x in corpus_context_VAL_jap]\n",
    "q = tfidf_vectorizer_questions.transform(corpus_questions_VAL_jap_not_t) #ignores unknown words\n",
    "c = tfidf_vectorizer_context.transform(corpus_context_VAL_jap_not_t) #ignores unknown words\n",
    "\n",
    "X_VAL = hstack([q,c])\n",
    "\n",
    "y = [] #-1 not answ, 1 is answ.\n",
    "Y_VAL = []\n",
    "for i in jap_training:\n",
    "    y.append(jap_training[i]['label'])\n",
    "    \n",
    "for i in jap_val:\n",
    "    Y_VAL.append(jap_val[i]['label'])\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "#training accuracy score\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80262d96",
   "metadata": {},
   "source": [
    "#### maybe the problem is due to the tokenizer used in tfidf model, but I wasn't able to use janome..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7288cdc",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2ace1",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff873240",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words): CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfbbe307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b29397",
   "metadata": {},
   "source": [
    "#### The model is trained on single words but I will get an avg of all the words rep for each sentence in order to obtain a sentence representation and not a single word one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe9f7c",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d884de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_count= ignore words with less than min_count value\n",
    "#window = window used for CBOW model\n",
    "import gensim\n",
    "vector_size = 500\n",
    "continous_rep_eng_questions = gensim.models.Word2Vec(corpus_questions_eng, min_count = 1,\n",
    "                              vector_size = vector_size, window = 5, epochs=5) \n",
    "continous_rep_eng_context = gensim.models.Word2Vec(corpus_context_eng, min_count = 1,\n",
    "                              vector_size = vector_size, window = 5, epochs=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d1b7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of representations of input level\n",
    "for i in eng_training:\n",
    "    sent = eng_training[i]['question']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        average_vector += continous_rep_eng_questions.wv[word]\n",
    "    average_vector = average_vector / n\n",
    "    eng_training[i]['CBOW_question'] = average_vector\n",
    "\n",
    "for i in eng_training:\n",
    "    sent = eng_training[i]['context']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        average_vector += continous_rep_eng_context.wv[word]\n",
    "    average_vector = average_vector / n\n",
    "    eng_training[i]['CBOW_context'] = average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "783716b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eng_val:\n",
    "    count_not_in_vocab = 0\n",
    "    sent = eng_val[i]['question']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        try:\n",
    "            average_vector += continous_rep_eng_questions.wv[word]\n",
    "        except:\n",
    "            count_not_in_vocab += 1\n",
    "            #print(\"word not in vocabulary\")\n",
    "    average_vector = average_vector / (n - count_not_in_vocab)\n",
    "    eng_val[i]['CBOW_question'] = average_vector\n",
    "    \n",
    "for i in eng_val:\n",
    "    count_not_in_vocab = 0\n",
    "    sent = eng_val[i]['context']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        try:\n",
    "            average_vector += continous_rep_eng_context.wv[word]\n",
    "        except:\n",
    "            count_not_in_vocab += 1\n",
    "            #print(\"word not in vocabulary\")\n",
    "    if n - count_not_in_vocab >0:\n",
    "        average_vector = average_vector / (n - count_not_in_vocab)\n",
    "    else:\n",
    "        average_vector = average_vector/1\n",
    "    eng_val[i]['CBOW_context'] = average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a658f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = [] #0 not answ, 1 is answ.\n",
    "X_VAL = []\n",
    "Y_VAL = []\n",
    "for i in eng_training:\n",
    "    X.append(list(np.concatenate((eng_training[i]['CBOW_question'],eng_training[i]['CBOW_context']),axis = None)))\n",
    "    y.append(eng_training[i]['label'])\n",
    "    \n",
    "for i in eng_val:\n",
    "    X_VAL.append(list(np.concatenate((eng_val[i]['CBOW_question'],eng_val[i]['CBOW_context']),axis = None)))\n",
    "    Y_VAL.append(eng_val[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84de6469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7389x54640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 505912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fbc2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "CBOW_rep = csr_matrix(np.array(X))\n",
    "#lots of elements because it's not sparse... but in this way I can stack it to the previous rep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e8023b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([features_eng,CBOW_rep]) #features_eng = features of prev. representations, CBOW_rep current rep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2db18453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7389x55640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7894912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X # +200 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a38e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "CBOW_rep_val = csr_matrix(np.array(X_VAL))\n",
    "X_VAL = hstack([features_val_eng,CBOW_rep_val]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc90e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "86.46636892678305\n",
      "vald_set accuracy\n",
      "72.32323232323232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "LR = LogisticRegression(max_iter=1000)\n",
    "\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0d360",
   "metadata": {},
   "source": [
    "## just CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51ab549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = [] #0 not answ, 1 is answ.\n",
    "X_VAL = []\n",
    "Y_VAL = []\n",
    "for i in eng_training:\n",
    "    X.append(list(np.concatenate((eng_training[i]['CBOW_question'],eng_training[i]['CBOW_context']),axis = None)))\n",
    "    y.append(eng_training[i]['label'])\n",
    "    \n",
    "for i in eng_val:\n",
    "    X_VAL.append(list(np.concatenate((eng_val[i]['CBOW_question'],eng_val[i]['CBOW_context']),axis = None)))\n",
    "    Y_VAL.append(eng_val[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33cc67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "69.84706996887265\n",
      "vald_set accuracy\n",
      "65.85858585858585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "LR = LogisticRegression(random_state=0, max_iter=1000)\n",
    "#training decision tree\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc950665",
   "metadata": {},
   "source": [
    "# Repeat cbow for finn and jap. ? or one classifier is enough to show?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e1516",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812de7d",
   "metadata": {},
   "source": [
    "#### Re-formatting data for the transformers use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08d727bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_transformers = {'question' : [], 'context': []}\n",
    "for i in eng_training:\n",
    "    eng_transformers['question'].append(\" \".join(eng_training[i]['question']))\n",
    "    eng_transformers['context'].append(\" \".join(eng_training[i]['context']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319bbf0d",
   "metadata": {},
   "source": [
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2a7d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"name\": \"Roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"name\": \"Roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at deepset/roberta-base-squad2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "loading configuration file config.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"name\": \"Roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"name\": \"Roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\fiori/.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2\\snapshots\\65f7840c86b02ca4df86024defd6de99fcf1fc10\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"deepset/roberta-base-squad2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"language\": \"english\",\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"name\": \"Roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name, tokenizer\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m      7\u001b[0m QA_input \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m:[ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhy is model conversion important?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond question?\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond asnwer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m }\n\u001b[1;32m---> 11\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_transformers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# b) Load model & tokenizer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\question_answering.py:380\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[1;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[1;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\pt_utils.py:111\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\pt_utils.py:253\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 253\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\question_answering.py:500\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    498\u001b[0m example \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    499\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {k: inputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[1;32m--> 500\u001b[0m start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: end, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1517\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1517\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1531\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:528\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    519\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    520\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    521\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    525\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    526\u001b[0m     )\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 528\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:455\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    452\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    453\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 455\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:468\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    467\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 468\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:379\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    381\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question':[ 'Why is model conversion important?', 'second question?'], \n",
    "    'context': ['The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.', 'second asnwer'],\n",
    "}\n",
    "res = nlp(eng_transformers)\n",
    "print(res)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41fe7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df930dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad51059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ffd90c4",
   "metadata": {},
   "source": [
    "### Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb67632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, QuestionAnsweringPipeline, pipeline\n",
    "\n",
    "model_name = \"ilmariky/bert-base-finnish-cased-squad2-fi\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Mikä tämä on?',\n",
    "    'context': 'Tämä on testi.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "train_res = nlp(finn_training)\n",
    "print(res)\n",
    "print(train_res)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffd03cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69945e87",
   "metadata": {},
   "source": [
    "### Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,QuestionAnsweringPipeline\n",
    "\n",
    "model_name = \"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\"\n",
    "\n",
    "# a) Get predictions\n",
    "QA_input = {\n",
    "    'question': '国語',\n",
    "    'context': '全学年にわたって小学校の国語の教科書に挿し絵>が用いられている'\n",
    "}\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Option 1:\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "res = nlp(QA_input)\n",
    "train_res = nlp(jap_training)\n",
    "print(res)\n",
    "print(train_res)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "# Option 2:\n",
    "nlp2 = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer, align_to_words=False)\n",
    "res2 = nlp2(QA_input)\n",
    "train_res2 = nlp2(jap_training)\n",
    "print(res)\n",
    "print(train_res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edf578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7bf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578f78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fedbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39212a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f977f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
