{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77650b3",
   "metadata": {},
   "source": [
    "# 1st task, dataset exploration and binary calssification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc72f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.5.1-py3-none-any.whl (431 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.20.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Using cached fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-macosx_10_9_x86_64.whl (34 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp38-cp38-macosx_10_13_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 889 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.62.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: packaging in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from datasets) (20.9)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Collecting dill<0.3.6\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-macosx_10_9_x86_64.whl (359 kB)\n",
      "\u001b[K     |████████████████████████████████| 359 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-macosx_10_9_x86_64.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 14.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-macosx_10_9_x86_64.whl (36 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /Users/gonzalovillalbamartinez/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, charset-normalizer, async-timeout, aiosignal, tqdm, fsspec, dill, aiohttp, xxhash, responses, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 charset-normalizer-2.1.1 datasets-2.5.1 dill-0.3.5.1 frozenlist-1.3.1 fsspec-2022.8.2 huggingface-hub-0.10.0 multidict-6.0.2 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 tqdm-4.64.1 xxhash-3.0.0 yarl-1.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using custom data configuration copenlu--nlp_course_tydiqa-cceecfb5416d988a\n",
      "Found cached dataset parquet (/Users/gonzalovillalbamartinez/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c508b3fa4e154de294c7c7ad102e8c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4236bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(train_set)#.sample(frac=1) #RESHUFFLING\n",
    "vs = pd.DataFrame(validation_set)#.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166d770d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116062</th>\n",
       "      <td>Kapan Kaisar Tang Gaozu mulai menjabat ?</td>\n",
       "      <td>Kaisar Tang Gaozu</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Hingga tahun 626, situasi makin memanas, Li Sh...</td>\n",
       "      <td>https://id.wikipedia.org/wiki/Kaisar%20Tang%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116063</th>\n",
       "      <td>من ابتكر المثلجات؟</td>\n",
       "      <td>مثلجات</td>\n",
       "      <td>arabic</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>وأشار هؤلاء الإخصائيون إلى أن “صداع الآيس كريم...</td>\n",
       "      <td>https://ar.wikipedia.org/wiki/%D9%85%D8%AB%D9%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116064</th>\n",
       "      <td>বাংলা ব্যাকরণ মতে বিশেষণ কয় প্রকার ?</td>\n",
       "      <td>বিশেষণ</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>উপমান: যার সাথে তুলনা করা হয়।\\nউপমেয়: যাকে ত...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116065</th>\n",
       "      <td>ブラームスの出身はどこ？</td>\n",
       "      <td>ヨハネス・ブラームス</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116066</th>\n",
       "      <td>What is the population of Mahwah, NJ?</td>\n",
       "      <td>Mahwah, New Jersey</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>The previous mayor, Bill Laforet faced a recal...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahwah%2C%20New%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116067 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   question_text      document_title  \\\n",
       "0                   Milloin Charles Fort syntyi?        Charles Fort   \n",
       "1                    “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ        ダニエル・J・キャラハン   \n",
       "2         వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?                 వేప   \n",
       "3             চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?         চেঙ্গিজ খান   \n",
       "4               రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?          రెయ్యలగడ్ద   \n",
       "...                                          ...                 ...   \n",
       "116062  Kapan Kaisar Tang Gaozu mulai menjabat ?   Kaisar Tang Gaozu   \n",
       "116063                        من ابتكر المثلجات؟              مثلجات   \n",
       "116064      বাংলা ব্যাকরণ মতে বিশেষণ কয় প্রকার ?              বিশেষণ   \n",
       "116065                              ブラームスの出身はどこ？          ヨハネス・ブラームス   \n",
       "116066     What is the population of Mahwah, NJ?  Mahwah, New Jersey   \n",
       "\n",
       "          language                                        annotations  \\\n",
       "0          finnish  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1         japanese  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2           telugu  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3          bengali  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4           telugu  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "...            ...                                                ...   \n",
       "116062  indonesian        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116063      arabic        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116064     bengali        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116065    japanese        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116066     english        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                       document_plaintext  \\\n",
       "0       Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1       “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2       వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3       চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4       రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "...                                                   ...   \n",
       "116062  Hingga tahun 626, situasi makin memanas, Li Sh...   \n",
       "116063  وأشار هؤلاء الإخصائيون إلى أن “صداع الآيس كريم...   \n",
       "116064  উপমান: যার সাথে তুলনা করা হয়।\\nউপমেয়: যাকে ত...   \n",
       "116065  \\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...   \n",
       "116066  The previous mayor, Bill Laforet faced a recal...   \n",
       "\n",
       "                                             document_url  \n",
       "0            https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1       https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2       https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3       https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4       https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  \n",
       "...                                                   ...  \n",
       "116062  https://id.wikipedia.org/wiki/Kaisar%20Tang%20...  \n",
       "116063  https://ar.wikipedia.org/wiki/%D9%85%D8%AB%D9%...  \n",
       "116064  https://bn.wikipedia.org/wiki/%E0%A6%AC%E0%A6%...  \n",
       "116065  https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...  \n",
       "116066  https://en.wikipedia.org/wiki/Mahwah%2C%20New%...  \n",
       "\n",
       "[116067 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c561824d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = []\n",
    "new_vs = []\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i,2] == 'japanese' or df.iloc[i,2] == 'finnish' or df.iloc[i,2] == 'english':\n",
    "        new_df.append(df.iloc[i, :])\n",
    "for i in range(len(vs)):\n",
    "    if vs.iloc[i,2] == 'japanese' or vs.iloc[i,2] == 'finnish' or vs.iloc[i,2] == 'english':\n",
    "        new_vs.append(vs.iloc[i, :])\n",
    "df = pd.DataFrame(new_df) #only with english, japanese or finnish text\n",
    "vs = pd.DataFrame(new_vs) #only with english, japanese or finnish text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5595f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mitä on altruismi?</td>\n",
       "      <td>Altruismi</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [44], 'answer_text': ['epäits...</td>\n",
       "      <td>\\n\\n\\nAltruismi ([1],  ”toinen”[2]) tarkoittaa...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Altruismi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mikä oli Wilhelm Wagner viimeinen sävellys?</td>\n",
       "      <td>Richard Wagner</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [597], 'answer_text': ['Parsi...</td>\n",
       "      <td>Wagnerin mestariteoksia ovat hänen myöhäiskaud...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Richard%20Wagner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Missä Harz sijaitsee?</td>\n",
       "      <td>Harz</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [25], 'answer_text': ['Pohjoi...</td>\n",
       "      <td>\\n\\nHarz on horstivuoristo Pohjois-Saksassa[1]...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Harz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116055</th>\n",
       "      <td>Who developed the first thermonuclear weapon?</td>\n",
       "      <td>History of nuclear weapons</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>In the end, President Truman made the final de...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/History%20of%20n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116059</th>\n",
       "      <td>ギュスターヴ・シャルパンティエはいつ生まれた？</td>\n",
       "      <td>ギュスターヴ・シャルパンティエ</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>ただし、第一次世界大戦時に負傷兵のための音楽会を主宰し、自作の指揮も行うなど隠棲することはな...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%AE%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116060</th>\n",
       "      <td>彭 徳懐はいつ生まれた？</td>\n",
       "      <td>彭徳懐故居</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>彭徳懐故居（ほうとくかいこきょ）は、中華人民共和国の政治家で軍人（中華人民共和国元帥）だった...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%BD%AD%E5%BE%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116065</th>\n",
       "      <td>ブラームスの出身はどこ？</td>\n",
       "      <td>ヨハネス・ブラームス</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116066</th>\n",
       "      <td>What is the population of Mahwah, NJ?</td>\n",
       "      <td>Mahwah, New Jersey</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>The previous mayor, Bill Laforet faced a recal...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahwah%2C%20New%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29868 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        question_text  \\\n",
       "0                        Milloin Charles Fort syntyi?   \n",
       "1                         “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   \n",
       "10                                 Mitä on altruismi?   \n",
       "12        Mikä oli Wilhelm Wagner viimeinen sävellys?   \n",
       "13                              Missä Harz sijaitsee?   \n",
       "...                                               ...   \n",
       "116055  Who developed the first thermonuclear weapon?   \n",
       "116059                        ギュスターヴ・シャルパンティエはいつ生まれた？   \n",
       "116060                                   彭 徳懐はいつ生まれた？   \n",
       "116065                                   ブラームスの出身はどこ？   \n",
       "116066          What is the population of Mahwah, NJ?   \n",
       "\n",
       "                    document_title  language  \\\n",
       "0                     Charles Fort   finnish   \n",
       "1                     ダニエル・J・キャラハン  japanese   \n",
       "10                       Altruismi   finnish   \n",
       "12                  Richard Wagner   finnish   \n",
       "13                            Harz   finnish   \n",
       "...                            ...       ...   \n",
       "116055  History of nuclear weapons   english   \n",
       "116059             ギュスターヴ・シャルパンティエ  japanese   \n",
       "116060                       彭徳懐故居  japanese   \n",
       "116065                  ヨハネス・ブラームス  japanese   \n",
       "116066          Mahwah, New Jersey   english   \n",
       "\n",
       "                                              annotations  \\\n",
       "0       {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1       {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "10      {'answer_start': [44], 'answer_text': ['epäits...   \n",
       "12      {'answer_start': [597], 'answer_text': ['Parsi...   \n",
       "13      {'answer_start': [25], 'answer_text': ['Pohjoi...   \n",
       "...                                                   ...   \n",
       "116055        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116059        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116060        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116065        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "116066        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                       document_plaintext  \\\n",
       "0       Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1       “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "10      \\n\\n\\nAltruismi ([1],  ”toinen”[2]) tarkoittaa...   \n",
       "12      Wagnerin mestariteoksia ovat hänen myöhäiskaud...   \n",
       "13      \\n\\nHarz on horstivuoristo Pohjois-Saksassa[1]...   \n",
       "...                                                   ...   \n",
       "116055  In the end, President Truman made the final de...   \n",
       "116059  ただし、第一次世界大戦時に負傷兵のための音楽会を主宰し、自作の指揮も行うなど隠棲することはな...   \n",
       "116060  彭徳懐故居（ほうとくかいこきょ）は、中華人民共和国の政治家で軍人（中華人民共和国元帥）だった...   \n",
       "116065  \\nベートーヴェンと同様に自然を愛好し、よくウィーン周辺の森を散策した。その際にキャンディを...   \n",
       "116066  The previous mayor, Bill Laforet faced a recal...   \n",
       "\n",
       "                                             document_url  \n",
       "0            https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1       https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "10                https://fi.wikipedia.org/wiki/Altruismi  \n",
       "12         https://fi.wikipedia.org/wiki/Richard%20Wagner  \n",
       "13                     https://fi.wikipedia.org/wiki/Harz  \n",
       "...                                                   ...  \n",
       "116055  https://en.wikipedia.org/wiki/History%20of%20n...  \n",
       "116059  https://ja.wikipedia.org/wiki/%E3%82%AE%E3%83%...  \n",
       "116060  https://ja.wikipedia.org/wiki/%E5%BD%AD%E5%BE%...  \n",
       "116065  https://ja.wikipedia.org/wiki/%E3%83%A8%E3%83%...  \n",
       "116066  https://en.wikipedia.org/wiki/Mahwah%2C%20New%...  \n",
       "\n",
       "[29868 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e528c32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gonzalovillalbamartinez/opt/miniconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "# !which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f37dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: joblib in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp38-cp38-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/gonzalovillalbamartinez/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Installing collected packages: regex, nltk\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2021.4.4\n",
      "    Uninstalling regex-2021.4.4:\n",
      "      Successfully uninstalled regex-2021.4.4\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.6.1\n",
      "    Uninstalling nltk-3.6.1:\n",
      "      Successfully uninstalled nltk-3.6.1\n",
      "Successfully installed nltk-3.7 regex-2022.9.13\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Installing collected packages: janome\n",
      "Successfully installed janome-0.4.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install -U nltk\n",
    "# !pip install janome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "288ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer as tok_jap #japanese tokenizer\n",
    "tok_jap = tok_jap()\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize #english tokenizer\n",
    "#[token for token in t.tokenize(df.iloc[1,4], wakati=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788ab3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gonzalovillalbamartinez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e86799c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Milloin', 'Charles', 'Fort', 'syntyi', '?']\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(df.iloc[0, 0])\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d9b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29868/29868 [03:06<00:00, 160.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_answers = {} #dictionary, key = number of iteration in the dataframe, value = answer_text, or '' if it's not answerable\n",
    "tokenized_questions = {} #same as before but with questions\n",
    "tokenized_documents = {} #same but with documents\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    if df.iloc[i,2] == 'japanese': #check the language\n",
    "        tokenized_answers[i] = [token for token in tok_jap.tokenize(df.iloc[i,3]['answer_text'][0], wakati=True)]\n",
    "        tokenized_questions[i] = [token for token in tok_jap.tokenize(df.iloc[i,0], wakati=True)]\n",
    "        tokenized_documents[i] = [token for token in tok_jap.tokenize(df.iloc[i,4], wakati=True)]\n",
    "    if df.iloc[i,2] == 'english':\n",
    "        tokenized_answers[i] = word_tokenize(df.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions[i] = word_tokenize(df.iloc[i,0])\n",
    "        tokenized_documents[i] = word_tokenize(df.iloc[i,4])\n",
    "    if df.iloc[i,2] == 'finnish': #I used the same tokenizer, maybe we can find another one that is better for finnish\n",
    "        tokenized_answers[i] = word_tokenize(df.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions[i] = word_tokenize(df.iloc[i,0])\n",
    "        tokenized_documents[i] = word_tokenize(df.iloc[i,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9ef9381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3712/3712 [00:19<00:00, 186.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tokenized_answers_validation = {} #dictionary, key = number of iteration in the dataframe, value = answer_text, or '' if it's not answerable\n",
    "tokenized_questions_validation = {} #same as before but with questions\n",
    "tokenized_documents_validation = {} #same but with documents\n",
    "\n",
    "for i in tqdm(range(len(vs))):\n",
    "    if vs.iloc[i,2] == 'japanese': #check the language\n",
    "        tokenized_answers_validation[i] = [token for token in tok_jap.tokenize(vs.iloc[i,3]['answer_text'][0], wakati=True)]\n",
    "        tokenized_questions_validation[i] = [token for token in tok_jap.tokenize(vs.iloc[i,0], wakati=True)]\n",
    "        tokenized_documents_validation[i] = [token for token in tok_jap.tokenize(vs.iloc[i,4], wakati=True)]\n",
    "    if vs.iloc[i,2] == 'english':\n",
    "        tokenized_answers_validation[i] = word_tokenize(vs.iloc[i,3]['answer_text'][0])\n",
    "        tokenized_questions_validation[i] = word_tokenize(vs.iloc[i,0])\n",
    "        tokenized_documents_validation[i] = word_tokenize(vs.iloc[i,4])\n",
    "    # if vs.iloc[i,2] == 'finnish': #I used the same tokenizer, maybe we can find another one that is better for finnish\n",
    "    #     tokenized_answers_validation[i] = word_tokenize(vs.iloc[i,3]['answer_text'][0])\n",
    "    #     tokenized_questions_validation[i] = word_tokenize(vs.iloc[i,0])\n",
    "    #     tokenized_documents_validation[i] = word_tokenize(vs.iloc[i,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14a9203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>化学兵器禁止条約はどこで採択された？</td>\n",
       "      <td>化学兵器禁止条約</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [11], 'answer_text': ['パリ']}</td>\n",
       "      <td>1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>オリヴィア・デ・ハヴィランドが生まれたのはいつ</td>\n",
       "      <td>オリヴィア・デ・ハヴィランド</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [46], 'answer_text': ['1916年7...</td>\n",
       "      <td>\\nオリヴィア・デ・ハヴィランド（Dame Olivia De Havilland, DBE...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Kauanko lasia on valmistettu?</td>\n",
       "      <td>Lasi</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [160], 'answer_text': ['noin ...</td>\n",
       "      <td>Vanhin tunnettu lasilaatu on alkali­kalkki­las...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Lasi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mikä on Ponzi-huijaus?</td>\n",
       "      <td>Ponzi-huijaus</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [39], 'answer_text': ['pyrami...</td>\n",
       "      <td>Ponzi-huijaus eli Ponzi-järjestelmä on pyramid...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Ponzi-huijaus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mikä oli Napoleonin sotien lopputulos?</td>\n",
       "      <td>Napoleonin sodat</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [15], 'answer_text': ['Ranska...</td>\n",
       "      <td>Sotien jälkeen Ranska alistettiin kovilla rauh...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Napoleonin%20sodat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13305</th>\n",
       "      <td>What is the most common first word by babies?</td>\n",
       "      <td>Vocabulary development</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Social pragmatic theories, also in contrast to...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Vocabulary%20dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>Kuinka kauan valtiopäivät kestää?</td>\n",
       "      <td>Suomen valtiopäivät</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>\\nEnnen vuotta 1906 kokoontuivat säätyvaltiopä...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Suomen%20valtiop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13315</th>\n",
       "      <td>アイスランド共和国の首都はどこですか？</td>\n",
       "      <td>アイスランド</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>こうした危機を乗り切るため、アイスランド中央銀行は8日にロシアから40億ユーロの緊急融資を受...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>Milloin käytiin Persianlahden sota?</td>\n",
       "      <td>Persianlahden sota</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Ilmaiskuilla oli erityisen tuhoisa vaikutus Ir...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Persianlahden%20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13319</th>\n",
       "      <td>When did the Bundaberg Central State School be...</td>\n",
       "      <td>Bundaberg Central State School</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>By the 1880s the school site had become valuab...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bundaberg%20Cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3712 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question_text  \\\n",
       "3                                     化学兵器禁止条約はどこで採択された？   \n",
       "9                                オリヴィア・デ・ハヴィランドが生まれたのはいつ   \n",
       "11                         Kauanko lasia on valmistettu?   \n",
       "14                                Mikä on Ponzi-huijaus?   \n",
       "20                Mikä oli Napoleonin sotien lopputulos?   \n",
       "...                                                  ...   \n",
       "13305      What is the most common first word by babies?   \n",
       "13307                  Kuinka kauan valtiopäivät kestää?   \n",
       "13315                                アイスランド共和国の首都はどこですか？   \n",
       "13317                Milloin käytiin Persianlahden sota?   \n",
       "13319  When did the Bundaberg Central State School be...   \n",
       "\n",
       "                       document_title  language  \\\n",
       "3                            化学兵器禁止条約  japanese   \n",
       "9                      オリヴィア・デ・ハヴィランド  japanese   \n",
       "11                               Lasi   finnish   \n",
       "14                      Ponzi-huijaus   finnish   \n",
       "20                   Napoleonin sodat   finnish   \n",
       "...                               ...       ...   \n",
       "13305          Vocabulary development   english   \n",
       "13307             Suomen valtiopäivät   finnish   \n",
       "13315                          アイスランド  japanese   \n",
       "13317              Persianlahden sota   finnish   \n",
       "13319  Bundaberg Central State School   english   \n",
       "\n",
       "                                             annotations  \\\n",
       "3          {'answer_start': [11], 'answer_text': ['パリ']}   \n",
       "9      {'answer_start': [46], 'answer_text': ['1916年7...   \n",
       "11     {'answer_start': [160], 'answer_text': ['noin ...   \n",
       "14     {'answer_start': [39], 'answer_text': ['pyrami...   \n",
       "20     {'answer_start': [15], 'answer_text': ['Ranska...   \n",
       "...                                                  ...   \n",
       "13305        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13307        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13315        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13317        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "13319        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                      document_plaintext  \\\n",
       "3      1993年1月13日にパリにおいて署名がなされ、1997年4月29日に発効した[1]。実効的...   \n",
       "9      \\nオリヴィア・デ・ハヴィランド（Dame Olivia De Havilland, DBE...   \n",
       "11     Vanhin tunnettu lasilaatu on alkali­kalkki­las...   \n",
       "14     Ponzi-huijaus eli Ponzi-järjestelmä on pyramid...   \n",
       "20     Sotien jälkeen Ranska alistettiin kovilla rauh...   \n",
       "...                                                  ...   \n",
       "13305  Social pragmatic theories, also in contrast to...   \n",
       "13307  \\nEnnen vuotta 1906 kokoontuivat säätyvaltiopä...   \n",
       "13315  こうした危機を乗り切るため、アイスランド中央銀行は8日にロシアから40億ユーロの緊急融資を受...   \n",
       "13317  Ilmaiskuilla oli erityisen tuhoisa vaikutus Ir...   \n",
       "13319  By the 1880s the school site had become valuab...   \n",
       "\n",
       "                                            document_url  \n",
       "3      https://ja.wikipedia.org/wiki/%E5%8C%96%E5%AD%...  \n",
       "9      https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%...  \n",
       "11                    https://fi.wikipedia.org/wiki/Lasi  \n",
       "14           https://fi.wikipedia.org/wiki/Ponzi-huijaus  \n",
       "20      https://fi.wikipedia.org/wiki/Napoleonin%20sodat  \n",
       "...                                                  ...  \n",
       "13305  https://en.wikipedia.org/wiki/Vocabulary%20dev...  \n",
       "13307  https://fi.wikipedia.org/wiki/Suomen%20valtiop...  \n",
       "13315  https://ja.wikipedia.org/wiki/%E3%82%A2%E3%82%...  \n",
       "13317  https://fi.wikipedia.org/wiki/Persianlahden%20...  \n",
       "13319  https://en.wikipedia.org/wiki/Bundaberg%20Cent...  \n",
       "\n",
       "[3712 rows x 6 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194d9f8",
   "metadata": {},
   "source": [
    "# Creating trainset and validation set for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b554f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#need only question and document, I don't really need the tokenization done before:\n",
    "jap_training = {} \n",
    "finn_training = {}\n",
    "eng_training = {}\n",
    "\n",
    "j = 0\n",
    "f = 0\n",
    "e = 0\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    sample_dict = {}\n",
    "    sample_dict['question_tokenized'] = tokenized_questions[i]\n",
    "    sample_dict['context'] = tokenized_documents[i]\n",
    "   \n",
    "    #sample_dict['question'] = df.iloc[i,0]\n",
    "    #sample_dict['document'] = df.iloc[i,4]\n",
    "    \n",
    "    if tokenized_answers[i] == [] :\n",
    "        sample_dict['label'] = -1 #not answerable question\n",
    "    else:\n",
    "        sample_dict['label'] = 1\n",
    "        \n",
    "    if df.iloc[i,2] == 'japanese':\n",
    "        jap_training[j] = sample_dict\n",
    "        j += 1\n",
    "        continue\n",
    "    elif df.iloc[i,2] == 'english':\n",
    "        eng_training[e] = sample_dict\n",
    "        e += 1\n",
    "        continue\n",
    "    elif df.iloc[i,2] == 'finnish':\n",
    "        finn_training[f] = sample_dict\n",
    "        f += 1\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7562423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'founded', 'the', 'Burntisland', 'Shipbuilding', 'Company', '?']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#validation set\n",
    "import nltk\n",
    "\n",
    "jap_val = {} \n",
    "finn_val = {}\n",
    "eng_val = {}\n",
    "\n",
    "j = 0\n",
    "f = 0\n",
    "e = 0\n",
    "\n",
    "sample_dict['question_tokenized'] = tokenized_questions_validation[10]\n",
    "print(sample_dict['question_tokenized'])\n",
    "\n",
    "for i in range(len(vs)-1):\n",
    "    sample_dict = {}\n",
    "    print(i)\n",
    "    sample_dict['question_tokenized'] = tokenized_questions_validation[i]\n",
    "    sample_dict['context'] = tokenized_documents_validation[i]\n",
    "    \n",
    "    if tokenized_answers_validation[i] == [] :\n",
    "        sample_dict['label'] = -1 #not answerable question\n",
    "    else:\n",
    "        sample_dict['label'] = 1\n",
    "        \n",
    "    if vs.iloc[i,2] == 'japanese':\n",
    "        jap_val[j] = sample_dict\n",
    "        j += 1\n",
    "        continue\n",
    "    elif vs.iloc[i,2] == 'english':\n",
    "        eng_val[e] = sample_dict\n",
    "        e += 1\n",
    "        continue\n",
    "    elif vs.iloc[i,2] == 'finnish':\n",
    "        finn_val[f] = sample_dict\n",
    "        f += 1\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f508e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ec1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fca3583e",
   "metadata": {},
   "source": [
    "## Create corpus for bag of words (one for each lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da71b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus to create a vocabulary with all possible words\n",
    "\n",
    "corpus_questions_eng = []\n",
    "corpus_questions_finn = []\n",
    "corpus_questions_jap = []\n",
    "corpus_context_eng = []\n",
    "corpus_context_finn = []\n",
    "corpus_context_jap = []\n",
    "\n",
    "for x in eng_training:\n",
    "    corpus_questions_eng.append(eng_training[x]['question_tokenized'])\n",
    "    corpus_context_eng.append(eng_training[x]['context'])\n",
    "for x in finn_training:\n",
    "    corpus_questions_finn.append(finn_training[x]['question_tokenized'])\n",
    "    corpus_context_finn.append(finn_training[x]['context'])\n",
    "for x in jap_training:\n",
    "    corpus_questions_jap.append(jap_training[x]['question_tokenized'])\n",
    "    corpus_context_jap.append(jap_training[x]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ac13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_questions_VAL_eng = []\n",
    "corpus_context_VAL_eng = []\n",
    "corpus_questions_VAL_finn = []\n",
    "corpus_context_VAL_finn = []\n",
    "corpus_questions_VAL_jap = []\n",
    "corpus_context_VAL_jap = []\n",
    "\n",
    "for x in eng_val:\n",
    "    corpus_questions_VAL_eng.append(eng_val[x]['question_tokenized'])  \n",
    "    corpus_context_VAL_eng.append(eng_val[x]['context']) \n",
    "for x in finn_val:\n",
    "    corpus_questions_VAL_finn.append(finn_val[x]['question_tokenized'])  \n",
    "    corpus_context_VAL_finn.append(finn_val[x]['context']) \n",
    "for x in jap_val:\n",
    "    corpus_questions_VAL_jap.append(jap_val[x]['question_tokenized'])  \n",
    "    corpus_context_VAL_jap.append(jap_val[x]['context']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_dict_eng = {}\n",
    "bag_of_words_dict_finn = {}\n",
    "bag_of_words_dict_jap = {}\n",
    "\n",
    "for sent in corpus_questions_eng:\n",
    "    for word in sent:\n",
    "        if word not in bag_of_words_dict_eng:\n",
    "            bag_of_words_dict_eng[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_eng[word] +=1\n",
    "            \n",
    "for question in corpus_questions_finn:\n",
    "    for word in question:\n",
    "        if word not in bag_of_words_dict_finn:\n",
    "            bag_of_words_dict_finn[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_finn[word] +=1\n",
    "            \n",
    "for question in corpus_questions_jap:\n",
    "    for word in question:\n",
    "        if word not in bag_of_words_dict_jap:\n",
    "            bag_of_words_dict_jap[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_jap[word] +=1\n",
    "            \n",
    "#list_words = bag_of_words_dict_eng.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1de7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbag_of_words_dict_eng = {}\\nfor question in corpus_questions:\\n    for word in question:\\n        if word not in bag_of_words_dict_eng:\\n            bag_of_words_dict_eng[word] = 1\\n        else:\\n            bag_of_words_dict_eng[word] +=1\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VALIDATIO SET, the vocabulary should be the same as the training set\n",
    "#I have to use the same vocabulary as the training, even for the validation set, that's why the commented part of code it's not useful\n",
    "'''\n",
    "bag_of_words_dict_eng = {}\n",
    "for question in corpus_questions:\n",
    "    for word in question:\n",
    "        if word not in bag_of_words_dict_eng:\n",
    "            bag_of_words_dict_eng[word] = 1\n",
    "        else:\n",
    "            bag_of_words_dict_eng[word] +=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b566f53",
   "metadata": {},
   "source": [
    "### Top 10 words in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'the', 'When', 'was', 'What', 'is', 'of', 'How', 'in', 'did']\n"
     ]
    }
   ],
   "source": [
    "sorted_words_question = dict(sorted(bag_of_words_dict_eng.items(), key=lambda item:item[1], \n",
    "reverse=True))\n",
    "print(list(sorted_words_question)[:10]) #top 10 words in english questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fbfe2",
   "metadata": {},
   "source": [
    "### Top 10 words in finnish questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?', 'on', 'Milloin', 'Mikä', 'Missä', 'Kuka', 'oli', 'Mitä', 'syntyi', 'kuoli']\n"
     ]
    }
   ],
   "source": [
    "sorted_words_question = dict(sorted(bag_of_words_dict_finn.items(), key=lambda item:item[1], \n",
    "reverse=True))\n",
    "print(list(sorted_words_question)[:10]) #top 10 words in english questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7f9b2",
   "metadata": {},
   "source": [
    "### Top 10 words in japanese questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fde165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['は', 'の', '？', 'た', 'い', 'つ', '何', 'し', 'どこ', 'が']\n"
     ]
    }
   ],
   "source": [
    "sorted_words_question = dict(sorted(bag_of_words_dict_jap.items(), key=lambda item:item[1], \n",
    "reverse=True))\n",
    "print(list(sorted_words_question)[:10]) #top 10 words in english questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa41cd2",
   "metadata": {},
   "source": [
    "# TfIdf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd589a20",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d14ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_questions_eng_not_tokenized = [\" \".join(x) for x in corpus_questions_eng]\n",
    "corpus_context_eng_not_tokenized = [\" \".join(x) for x in corpus_context_eng]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2184ff",
   "metadata": {},
   "source": [
    "##### use tfidf for questions and then for context, concatenate the 2 and try to predict, for validation set use the same model of tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549d070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<7389x4603 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 50118 stored elements in Compressed Sparse Row format>,\n",
       " <7389x50037 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 455794 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_questions = TfidfVectorizer()\n",
    "q = tfidf_vectorizer_questions.fit_transform(corpus_questions_eng_not_tokenized)\n",
    "\n",
    "tfidf_vectorizer_context = TfidfVectorizer() #new model\n",
    "c = tfidf_vectorizer_context.fit_transform(corpus_context_eng_not_tokenized) \n",
    "q,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([q,c]) #questions and contexts tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7389x54640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 505912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baaa9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why not use fit_transform also on validation set:\n",
    "#https://stats.stackexchange.com/questions/154660/tfidfvectorizer-should-it-be-used-on-train-only-or-traintest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac13d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat process for validation set:\n",
    "corpus_questions_VAL_eng_not_t = [\" \".join(x) for x in corpus_questions_VAL_eng] # not tokenized\n",
    "corpus_context_VAL_eng_not_t = [\" \".join(x) for x in corpus_context_VAL_eng]\n",
    "q = tfidf_vectorizer_questions.transform(corpus_questions_VAL_eng_not_t) #ignores unknown words\n",
    "c = tfidf_vectorizer_context.transform(corpus_context_VAL_eng_not_t) #ignores unknown words\n",
    "\n",
    "X_VAL = hstack([q,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de2389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<990x54640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 63594 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f0c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [] #-1 not answ, 1 is answ.\n",
    "Y_VAL = []\n",
    "for i in eng_training:\n",
    "    y.append(eng_training[i]['label'])\n",
    "    \n",
    "for i in eng_val:\n",
    "    Y_VAL.append(eng_val[i]['label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741fe620",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      4\u001b[0m LR \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 5\u001b[0m LR\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[1;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m LR\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#training accuracy score\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "#training accuracy score\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59329d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_eng = X\n",
    "features_val_eng = X_VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94d034",
   "metadata": {},
   "source": [
    "Using pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe63a21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "train_res = nlp(eng_training)\n",
    "print(res)\n",
    "print(train_res)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ea9c18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, AutoModel\n\u001b[0;32m----> 3\u001b[0m AutoConfig\u001b[38;5;241m.\u001b[39mregister(model, LR)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mapply()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LR' is not defined"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoConfig, AutoModel\n",
    "\n",
    "# AutoConfig.register(model, LR)\n",
    "\n",
    "# model.apply(X_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45dc31",
   "metadata": {},
   "source": [
    "# Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17406dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "91.69403693161084\n",
      "vald_set accuracy\n",
      "71.23368920521945\n"
     ]
    }
   ],
   "source": [
    "#same as english\n",
    "corpus_questions_finn_not_tokenized = [\" \".join(x) for x in corpus_questions_finn]\n",
    "corpus_context_finn_not_tokenized = [\" \".join(x) for x in corpus_context_finn]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_questions = TfidfVectorizer()\n",
    "q = tfidf_vectorizer_questions.fit_transform(corpus_questions_finn_not_tokenized)\n",
    "\n",
    "tfidf_vectorizer_context = TfidfVectorizer() #new model\n",
    "c = tfidf_vectorizer_context.fit_transform(corpus_context_finn_not_tokenized) \n",
    "\n",
    "#concatenate sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([q,c]) #questions and contexts tfidfs\n",
    "\n",
    "corpus_questions_VAL_finn_not_t = [\" \".join(x) for x in corpus_questions_VAL_finn] # not tokenized\n",
    "corpus_context_VAL_finn_not_t = [\" \".join(x) for x in corpus_context_VAL_finn]\n",
    "q = tfidf_vectorizer_questions.transform(corpus_questions_VAL_finn_not_t) #ignores unknown words\n",
    "c = tfidf_vectorizer_context.transform(corpus_context_VAL_finn_not_t) #ignores unknown words\n",
    "\n",
    "X_VAL = hstack([q,c])\n",
    "\n",
    "y = [] #-1 not answ, 1 is answ.\n",
    "Y_VAL = []\n",
    "for i in finn_training:\n",
    "    y.append(finn_training[i]['label'])\n",
    "    \n",
    "for i in finn_val:\n",
    "    Y_VAL.append(finn_val[i]['label'])\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "#training accuracy score\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, QuestionAnsweringPipeline, pipeline\n",
    "\n",
    "model_name = \"ilmariky/bert-base-finnish-cased-squad2-fi\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Mikä tämä on?',\n",
    "    'context': 'Tämä on testi.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "train_res = nlp(finn_training)\n",
    "print(res)\n",
    "print(train_res)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f8e79",
   "metadata": {},
   "source": [
    "# Japanese\n",
    "##### overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef4230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "98.32535885167464\n",
      "vald_set accuracy\n",
      "57.91505791505791\n"
     ]
    }
   ],
   "source": [
    "#same as english\n",
    "corpus_questions_jap_not_tokenized = [\"\".join(x) for x in corpus_questions_jap] # no space in japanese\n",
    "corpus_context_jap_not_tokenized = [\"\".join(x) for x in corpus_context_jap]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_questions = TfidfVectorizer()\n",
    "q = tfidf_vectorizer_questions.fit_transform(corpus_questions_jap_not_tokenized)\n",
    "\n",
    "tfidf_vectorizer_context = TfidfVectorizer()\n",
    "c = tfidf_vectorizer_context.fit_transform(corpus_context_jap_not_tokenized) \n",
    "\n",
    "#concatenate sparse matrices\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([q,c]) #questions and contexts tfidfs\n",
    "\n",
    "corpus_questions_VAL_jap_not_t = [\" \".join(x) for x in corpus_questions_VAL_jap] # not tokenized\n",
    "corpus_context_VAL_jap_not_t = [\" \".join(x) for x in corpus_context_VAL_jap]\n",
    "q = tfidf_vectorizer_questions.transform(corpus_questions_VAL_jap_not_t) #ignores unknown words\n",
    "c = tfidf_vectorizer_context.transform(corpus_context_VAL_jap_not_t) #ignores unknown words\n",
    "\n",
    "X_VAL = hstack([q,c])\n",
    "\n",
    "y = [] #-1 not answ, 1 is answ.\n",
    "Y_VAL = []\n",
    "for i in jap_training:\n",
    "    y.append(jap_training[i]['label'])\n",
    "    \n",
    "for i in jap_val:\n",
    "    Y_VAL.append(jap_val[i]['label'])\n",
    "    \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "#training accuracy score\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fdf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForQuestionAnswering,QuestionAnsweringPipeline\n",
    "\n",
    "model_name = \"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\"\n",
    "\n",
    "# a) Get predictions\n",
    "QA_input = {\n",
    "    'question': '国語',\n",
    "    'context': '全学年にわたって小学校の国語の教科書に挿し絵>が用いられている'\n",
    "}\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Option 1:\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "res = nlp(QA_input)\n",
    "train_res = nlp(jap_training)\n",
    "print(res)\n",
    "print(train_res)\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "# Option 2:\n",
    "nlp2 = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer, align_to_words=False)\n",
    "res2 = nlp2(QA_input)\n",
    "train_res2 = nlp2(jap_training)\n",
    "print(res)\n",
    "print(train_res2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80262d96",
   "metadata": {},
   "source": [
    "#### maybe the problem is due to the tokenizer used in tfidf model, but I wasn't able to use janome..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7288cdc",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c2ace1",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff873240",
   "metadata": {},
   "source": [
    "##### CBOW (Continuous Bag of Words): CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbe307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b29397",
   "metadata": {},
   "source": [
    "#### The model is trained on single words but I will get an avg of all the words rep for each sentence in order to obtain a sentence representation and not a single word one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe9f7c",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_count= ignore words with less than min_count value\n",
    "#window = window used for CBOW model\n",
    "import gensim\n",
    "vector_size = 500\n",
    "continous_rep_eng_questions = gensim.models.Word2Vec(corpus_questions_eng, min_count = 1,\n",
    "                              vector_size = vector_size, window = 5, epochs=5) \n",
    "continous_rep_eng_context = gensim.models.Word2Vec(corpus_context_eng, min_count = 1,\n",
    "                              vector_size = vector_size, window = 5, epochs=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of representations of input level\n",
    "for i in eng_training:\n",
    "    sent = eng_training[i]['question_tokenized']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        average_vector += continous_rep_eng_questions.wv[word]\n",
    "    average_vector = average_vector / n\n",
    "    eng_training[i]['CBOW_question'] = average_vector\n",
    "\n",
    "for i in eng_training:\n",
    "    sent = eng_training[i]['context']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        average_vector += continous_rep_eng_context.wv[word]\n",
    "    average_vector = average_vector / n\n",
    "    eng_training[i]['CBOW_context'] = average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783716b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eng_val:\n",
    "    count_not_in_vocab = 0\n",
    "    sent = eng_val[i]['question_tokenized']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        try:\n",
    "            average_vector += continous_rep_eng_questions.wv[word]\n",
    "        except:\n",
    "            count_not_in_vocab += 1\n",
    "            #print(\"word not in vocabulary\")\n",
    "    average_vector = average_vector / (n - count_not_in_vocab)\n",
    "    eng_val[i]['CBOW_question'] = average_vector\n",
    "    \n",
    "for i in eng_val:\n",
    "    count_not_in_vocab = 0\n",
    "    sent = eng_val[i]['context']\n",
    "    n = len(sent)\n",
    "    average_vector = np.zeros(vector_size)\n",
    "    for word in sent:\n",
    "        try:\n",
    "            average_vector += continous_rep_eng_context.wv[word]\n",
    "        except:\n",
    "            count_not_in_vocab += 1\n",
    "            #print(\"word not in vocabulary\")\n",
    "    if n - count_not_in_vocab >0:\n",
    "        average_vector = average_vector / (n - count_not_in_vocab)\n",
    "    else:\n",
    "        average_vector = average_vector/1\n",
    "    eng_val[i]['CBOW_context'] = average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a658f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = [] #0 not answ, 1 is answ.\n",
    "X_VAL = []\n",
    "Y_VAL = []\n",
    "for i in eng_training:\n",
    "    X.append(list(np.concatenate((eng_training[i]['CBOW_question'],eng_training[i]['CBOW_context']),axis = None)))\n",
    "    y.append(eng_training[i]['label'])\n",
    "    \n",
    "for i in eng_val:\n",
    "    X_VAL.append(list(np.concatenate((eng_val[i]['CBOW_question'],eng_val[i]['CBOW_context']),axis = None)))\n",
    "    Y_VAL.append(eng_val[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de6469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7389x54640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 505912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "CBOW_rep = csr_matrix(np.array(X))\n",
    "#lots of elements because it's not sparse... but in this way I can stack it to the previous rep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8023b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack([features_eng,CBOW_rep]) #features_eng = features of prev. representations, CBOW_rep current rep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db18453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7389x55640 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7894912 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X # +200 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "CBOW_rep_val = csr_matrix(np.array(X_VAL))\n",
    "X_VAL = hstack([features_val_eng,CBOW_rep_val]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "86.31749898497768\n",
      "vald_set accuracy\n",
      "72.32323232323232\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "LR = LogisticRegression(max_iter=1000)\n",
    "\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc0d360",
   "metadata": {},
   "source": [
    "## just CBOW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = [] #0 not answ, 1 is answ.\n",
    "X_VAL = []\n",
    "Y_VAL = []\n",
    "for i in eng_training:\n",
    "    X.append(list(np.concatenate((eng_training[i]['CBOW_question'],eng_training[i]['CBOW_context']),axis = None)))\n",
    "    y.append(eng_training[i]['label'])\n",
    "    \n",
    "for i in eng_val:\n",
    "    X_VAL.append(list(np.concatenate((eng_val[i]['CBOW_question'],eng_val[i]['CBOW_context']),axis = None)))\n",
    "    Y_VAL.append(eng_val[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set accuracy\n",
      "69.57639734740832\n",
      "vald_set accuracy\n",
      "65.45454545454545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "LR = LogisticRegression(random_state=0, max_iter=1000)\n",
    "#training decision tree\n",
    "LR.fit(X, y)\n",
    "y_pred = LR.predict(X)\n",
    "print(\"train_set accuracy\")\n",
    "print(accuracy_score(y, y_pred)*100)\n",
    "\n",
    "#accuracy score on val_set\n",
    "y_pred = LR.predict(X_VAL)\n",
    "print(\"vald_set accuracy\")\n",
    "print(accuracy_score(Y_VAL, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14fb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc950665",
   "metadata": {},
   "source": [
    "# Repeat cbow for finn and jap. ? or one classifier is enough to show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d006c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee2e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7d417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5c141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb732b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b3ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33556e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98edf578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7bf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578f78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7fedbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39212a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f977f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3460508f2b44ab2630f13d7f2c0e6bf5ef4b00662addea19d5333a21f8875a79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
